{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.card_engine import Card_Game, Card_Env, random_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        # save a transition\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    # n_input: the current state\n",
    "    #  (1x52)    +  (56x52)       +       (1x52): the current state\n",
    "    #    ^hand       ^who plays each card  ^cards not seen yet\n",
    "    #                       + cards played\n",
    "    # n_output: probability of playing each card\n",
    "    #   (1x52)\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_input, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dtype == torch.float32:\n",
    "            x = F.relu(self.layer1(x))\n",
    "            x = F.relu(self.layer2(x))\n",
    "        else:\n",
    "            x=x.to(torch.float32)\n",
    "            x = F.relu(self.layer1(x))\n",
    "            x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The network agent\n",
    "Selects a move according to epsilon-greedy policy:\n",
    "sometimes uses the model to select move, sometimes just select one randomally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A single step optimization of the model using Deep Q-Learning\n",
    "1) samples a batch from memory, concatenates all the tensors into a single one\n",
    "2) computes Q(s_t, a_t) and V(s_{t+1}) = max_a Q(s_{t+1}, a), where s_t --(a_t)--> s_{t+1}\n",
    "3) computes the loss\n",
    "4) updates the target network (which is computing V(s_{t+1})) at every step with soft update\n",
    "'''\n",
    "def optimize_model():\n",
    "    transitions = []\n",
    "    for turn, mem in memory.items():\n",
    "        if len(mem) >= BATCH_SIZE:\n",
    "            transitions += mem.sample(BATCH_SIZE)\n",
    "    if transitions == []:\n",
    "        return\n",
    "\n",
    "    # transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # mask the non final states and find the corresponding next states\n",
    "    # We need an illegal move to be a non-final state\n",
    "    # Right now, we are throwing out all the final states which include the case when\n",
    "    # the agent ends the game prematurely after playing an illegal move\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    # print('non_final_mask is', non_final_mask)\n",
    "    non_final_next_states = [s for s in batch.next_state if s is not None]\n",
    "    if non_final_next_states == []:\n",
    "        return\n",
    "    non_final_next_states = torch.cat(non_final_next_states)\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    # print('reward_batch is', reward_batch)\n",
    "    \n",
    "    \n",
    "    # compute Q(s_t, a)\n",
    "    # for each state in the batch, find the value of the corresponding action\n",
    "    state_action_values = policy_net(state_batch.to(torch.float)).gather(1, action_batch)\n",
    "    # compute V(s_{t+1}) = max_a Q(s_{t+1}, a) for the next_states using the target_net\n",
    "    next_state_values = torch.zeros(len(transitions), device=device)\n",
    "    with torch.no_grad():\n",
    "        # print(non_final_mask.shape, target_net(non_final_next_states).max(1).values\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "\n",
    "    # R + \\gamma max_a Q(s', a)\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # compute the Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # back propagate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. GPU can be used.\n",
      "Device name: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. GPU can be used.\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "env = Card_Env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "\n",
    "# I am making batch_size small here so that we can test if this goes through in shorter time\n",
    "BATCH_SIZE = 100\n",
    "MEMORY_SIZE = 10000\n",
    "\n",
    "# Learning rate of the optimizer\n",
    "LR = 1e-4\n",
    "\n",
    "# soft update rate\n",
    "TAU = 0.005\n",
    "\n",
    "# future discount\n",
    "GAMMA = 1.0\n",
    "\n",
    "\n",
    "state = env.game.get_network_input()\n",
    "\n",
    "n_input = len(state)\n",
    "n_actions = 52\n",
    "\n",
    "policy_net = DQN(n_input, n_actions).to(device)\n",
    "# use a target network to prevent oscillation or divergence\n",
    "target_net = DQN(n_input, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = {turn: ReplayMemory(MEMORY_SIZE) for turn in range(13)}\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "'''\n",
    "Given the game state, select an action by the epsilon-greedy policy\n",
    "'''\n",
    "def select_action(game):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "\n",
    "    # epsilon-greedy choice\n",
    "        \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # return the index of the card with highest probability\n",
    "            # predicted from the policy net\n",
    "            # print(policy_net(game.get_network_input().to(torch.float32).to(device)))\n",
    "            return policy_net(game.get_network_input().to(torch.float32).to(device)).max(0).indices.view(1,1)\n",
    "    else:\n",
    "        # random select a legal action\n",
    "        return torch.tensor([[game.sample_legal_move()]], device=device, dtype=torch.long) #changed from long\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the newest traing Q function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net.load_state_dict(torch.load('latest_q_function.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\evarv\\AppData\\Local\\Temp\\ipykernel_2108\\1006660112.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(env.game.get_network_input(), dtype=torch.float32, device=device).unsqueeze(0)\n",
      "C:\\Users\\evarv\\AppData\\Local\\Temp\\ipykernel_2108\\1006660112.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# fp =  DQN(n_input, 52).to(device)\n",
    "# fp.load_state_dict(torch.load('latest_q_function.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "num_episodes = 1000\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    state = torch.tensor(env.game.get_network_input(), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    player_ind = random.randint(0, 3)\n",
    "    while env.game.current_player != player_ind:\n",
    "        move = env.game.sample_legal_move()\n",
    "        env.game.play_card(move)\n",
    "    \n",
    "    for t in count():\n",
    "        # Select action based on policy network\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values = policy_net(state)\n",
    "            action = q_values.max(1)[1].view(1, 1)\n",
    "\n",
    "        # Perform action in the environment\n",
    "        observation, reward, terminated = env.step(action.item(),fp=None)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated\n",
    "\n",
    "        # Compute next state\n",
    "        if not terminated:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store transition in memory\n",
    "        # int(env.game.turn_counter / 4)\n",
    "        memory[t].push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform optimization step\n",
    "        optimize_model()\n",
    "\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            # print(f'Episode {i_episode} ended in {t} steps.')\n",
    "            break\n",
    "print('done')\n",
    "# Save the Q-function model at the end of training\n",
    "# torch.save(policy_net.state_dict(), 'latest_q_function.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1458, 309, 181, 73, 27, 9, 2, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(mem) for i, mem in memory.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the Q function for an initial state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "game = env.game\n",
    "state = torch.tensor(game.get_network_input(), dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "policy_net.eval()  # Set the network to evaluation mode\n",
    "q_values = policy_net(state)\n",
    "print(state)\n",
    "print(\"Initial Q-values:\", q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximal value corresponds to the right move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index of the maxima value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Q function for next training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), 'latest_q_function.pth')\n",
    "policy_net.load_state_dict(torch.load('latest_q_function.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

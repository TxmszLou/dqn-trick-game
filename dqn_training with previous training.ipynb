{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.card_engine import Card_Game, Card_Env, random_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        # save a transition\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    # n_input: the current state\n",
    "    #  (1x52)    +  (56x52)       +       (1x52): the current state\n",
    "    #    ^hand       ^who plays each card  ^cards not seen yet\n",
    "    #                       + cards played\n",
    "    # n_output: probability of playing each card\n",
    "    #   (1x52)\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_input, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dtype == torch.float32:\n",
    "            x = F.relu(self.layer1(x))\n",
    "            x = F.relu(self.layer2(x))\n",
    "        else:\n",
    "            x=x.to(torch.float32)\n",
    "            x = F.relu(self.layer1(x))\n",
    "            x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The network agent\n",
    "Selects a move according to epsilon-greedy policy:\n",
    "sometimes uses the model to select move, sometimes just select one randomally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A single step optimization of the model using Deep Q-Learning\n",
    "1) samples a batch from memory, concatenates all the tensors into a single one\n",
    "2) computes Q(s_t, a_t) and V(s_{t+1}) = max_a Q(s_{t+1}, a), where s_t --(a_t)--> s_{t+1}\n",
    "3) computes the loss\n",
    "4) updates the target network (which is computing V(s_{t+1})) at every step with soft update\n",
    "'''\n",
    "def optimize_model():\n",
    "    transitions = []\n",
    "    for turn, mem in memory.items():\n",
    "        if len(mem) >= BATCH_SIZE:\n",
    "            transitions += mem.sample(BATCH_SIZE)\n",
    "    if transitions == []:\n",
    "        return\n",
    "\n",
    "    # transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # mask the non final states and find the corresponding next states\n",
    "    # We need an illegal move to be a non-final state\n",
    "    # Right now, we are throwing out all the final states which include the case when\n",
    "    # the agent ends the game prematurely after playing an illegal move\n",
    "\n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    # print('non_final_mask is', non_final_mask)\n",
    "    non_final_next_states = [s for s in batch.next_state if s is not None]\n",
    "\n",
    "    non_final_next_states = torch.cat(non_final_next_states)\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    # print('reward_batch is', reward_batch)\n",
    "    \n",
    "    \n",
    "    # compute Q(s_t, a)\n",
    "    # for each state in the batch, find the value of the corresponding action\n",
    "    state_action_values = policy_net(state_batch.to(torch.float)).gather(1, action_batch)\n",
    "    \n",
    "    # compute V(s_{t+1}) = max_a Q(s_{t+1}, a) for the next_states using the target_net\n",
    "    next_state_values = torch.zeros(len(transitions), device=device)\n",
    "    # next_state_values = -10.0 * torch.ones(len(transitions), device=device)\n",
    "\n",
    "    if non_final_next_states != []:\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # print(non_final_mask.shape, target_net(non_final_next_states).max(1).values)\n",
    "            next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values  # these will all be negative!\n",
    "\n",
    "    # R + \\gamma max_a Q(s', a)\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # compute the Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # back propagate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. GPU can be used.\n",
      "Device name: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. GPU can be used.\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "env = Card_Env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_card(move):\n",
    "    return ['2','3','4','5','6','7','8','9','10','J','Q','K','A'][move % 13], ['C', 'D', 'H', 'S'][int(move / 13)]\n",
    "\n",
    "def simulate_game(policy, verbose=False, from_move=0):\n",
    "    with torch.no_grad():\n",
    "        moves_played = 0\n",
    "        active_player = from_move % 4\n",
    "        if verbose:\n",
    "            print(f\"Starting new game as player {active_player} from turn {from_move}.\")\n",
    "        test_game = Card_Game()\n",
    "        for turn in range(52):\n",
    "            if test_game.current_player != active_player or turn < from_move:\n",
    "                move = test_game.sample_legal_move()\n",
    "            else:\n",
    "                # print(policy_net(test_game.get_network_input().to(device)))\n",
    "                moves_played += 1\n",
    "                move = policy_net(test_game.get_network_input().to(device)).argmax().item()\n",
    "                if not(move in test_game.get_legal_moves()):\n",
    "                    if verbose:\n",
    "                        print(f\"Tried to play illegal move {move_to_card(move)}\")\n",
    "                    return moves_played\n",
    "            if verbose:\n",
    "                print(f\"Player {test_game.current_player} plays {move_to_card(move)}\")\n",
    "            test_game.play_card(move)\n",
    "            if turn % 4 == 3 and verbose:\n",
    "                print()\n",
    "        return moves_played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "\n",
    "# I am making batch_size small here so that we can test if this goes through in shorter time\n",
    "BATCH_SIZE = 100\n",
    "MEMORY_SIZE = 10000\n",
    "\n",
    "# Learning rate of the optimizer\n",
    "LR = 1e-4\n",
    "\n",
    "# soft update rate\n",
    "TAU = 0.005\n",
    "\n",
    "# future discount\n",
    "# GAMMA = 1.0\n",
    "GAMMA = 0\n",
    "\n",
    "\n",
    "state = env.game.get_network_input()\n",
    "\n",
    "n_input = len(state)\n",
    "n_actions = 52\n",
    "\n",
    "policy_net = DQN(n_input, n_actions).to(device)\n",
    "# use a target network to prevent oscillation or divergence\n",
    "target_net = DQN(n_input, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "# target_net.load_state_dict(torch.load('ev_working_function.pth'))\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = {turn: ReplayMemory(MEMORY_SIZE) for turn in range(13)}\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "'''\n",
    "Given the game state, select an action by the epsilon-greedy policy\n",
    "'''\n",
    "def select_action(game):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "\n",
    "    # epsilon-greedy choice\n",
    "        \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # return the index of the card with highest probability\n",
    "            # predicted from the policy net\n",
    "            # print(policy_net(game.get_network_input().to(torch.float32).to(device)))\n",
    "            return policy_net(game.get_network_input().to(torch.float32).to(device)).max(0).indices.view(1,1)\n",
    "    else:\n",
    "        # random select a legal action\n",
    "        return torch.tensor([[game.sample_legal_move()]], device=device, dtype=torch.long) #changed from long\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0407, -0.0332, -0.0790,  0.0465, -0.0660, -0.0715, -0.0573,  0.0063,\n",
      "         0.0153,  0.0771,  0.0285, -0.0536,  0.0822, -0.0819,  0.0109, -0.1007,\n",
      "        -0.0319,  0.0219,  0.0108,  0.0045,  0.0022,  0.0050,  0.1069,  0.0508,\n",
      "         0.0795, -0.0333,  0.0504,  0.0288, -0.0312, -0.0985, -0.0405, -0.0056,\n",
      "         0.0174,  0.0303, -0.0761,  0.0066,  0.0535, -0.0558, -0.0275,  0.0384,\n",
      "         0.0345,  0.0211, -0.0121, -0.0511, -0.0630,  0.0259,  0.0545,  0.0538,\n",
      "        -0.0268,  0.0404, -0.0394,  0.0149], device='cuda:0')\n",
      "tensor([ 0.0407, -0.0332, -0.0790,  0.0465, -0.0660, -0.0715, -0.0573,  0.0063,\n",
      "         0.0153,  0.0771,  0.0285, -0.0536,  0.0822, -0.0819,  0.0109, -0.1007,\n",
      "        -0.0319,  0.0219,  0.0108,  0.0045,  0.0022,  0.0050,  0.1069,  0.0508,\n",
      "         0.0795, -0.0333,  0.0504,  0.0288, -0.0312, -0.0985, -0.0405, -0.0056,\n",
      "         0.0174,  0.0303, -0.0761,  0.0066,  0.0535, -0.0558, -0.0275,  0.0384,\n",
      "         0.0345,  0.0211, -0.0121, -0.0511, -0.0630,  0.0259,  0.0545,  0.0538,\n",
      "        -0.0268,  0.0404, -0.0394,  0.0149], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(policy_net(env.game.get_network_input().to(device)))\n",
    "    print(target_net(env.game.get_network_input().to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\evarv\\AppData\\Local\\Temp\\ipykernel_10360\\2086175876.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(env.game.get_network_input(), dtype=torch.float32, device=device).unsqueeze(0)\n",
      "C:\\Users\\evarv\\AppData\\Local\\Temp\\ipykernel_10360\\2086175876.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained 5000 episodes\n",
      "Average reward per episode: -0.34120171673819744.\n",
      "Average simulated game duration: 1.79\n",
      "Distribution of game lengths: [62, 12, 15, 7, 4, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Memory bank: [10000, 10000, 9140, 3479, 844, 121, 10, 1, 0, 0, 0, 0, 0]\n",
      "\n",
      "Trained 10000 episodes\n",
      "Average reward per episode: -0.3051422605063952.\n",
      "Average simulated game duration: 1.84\n",
      "Distribution of game lengths: [63, 12, 10, 10, 3, 2, 0, 0, 0, 0, 0, 0, 0]\n",
      "Memory bank: [10000, 10000, 10000, 3923, 982, 151, 13, 2, 0, 0, 0, 0, 0]\n",
      "New benchmark set.\n",
      "\n",
      "Trained 15000 episodes\n",
      "Average reward per episode: -0.29819550824354146.\n",
      "Average simulated game duration: 1.97\n",
      "Distribution of game lengths: [53, 18, 15, 9, 3, 2, 0, 0, 0, 0, 0, 0, 0]\n",
      "Memory bank: [10000, 10000, 10000, 4376, 1136, 180, 18, 3, 0, 0, 0, 0, 0]\n",
      "New benchmark set.\n",
      "\n",
      "Trained 20000 episodes\n",
      "Average reward per episode: -0.3007284079084287.\n",
      "Average simulated game duration: 1.78\n",
      "Distribution of game lengths: [66, 10, 10, 9, 4, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "Memory bank: [10000, 10000, 10000, 4851, 1307, 217, 21, 3, 0, 0, 0, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fp =  DQN(n_input, 52).to(device)\n",
    "# fp.load_state_dict(torch.load('latest_q_function.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "update_ind = 5000\n",
    "\n",
    "rewards_list = []\n",
    "benchmark = (-9.6, 1)\n",
    "\n",
    "for i_episode in count():\n",
    "\n",
    "    if i_episode % update_ind == 0 and i_episode != 0:\n",
    "        print(f\"Trained {i_episode} episodes\")\n",
    "        avg_reward = sum(rewards_list) / len(rewards_list)\n",
    "        print(f'Average reward per episode: {avg_reward}.')\n",
    "        simul_results = []\n",
    "        simul_dist = [0 for i in range(13)]\n",
    "        for g in range(100):\n",
    "            res = simulate_game(policy_net, verbose=False, from_move=random.randint(0,3))\n",
    "            simul_results.append(res)\n",
    "            simul_dist[res-1] += 1\n",
    "        print(f\"Average simulated game duration: {sum(simul_results) / 100}\")\n",
    "        print(f\"Distribution of game lengths: {simul_dist}\")\n",
    "        print(f\"Memory bank: {[len(mem) for i, mem in memory.items()]}\")\n",
    "        if benchmark[0] < avg_reward and benchmark[1] < sum(simul_results) / 100 and i_episode >= 10000:\n",
    "            print(\"New benchmark set.\")\n",
    "            torch.save(policy_net.state_dict(), 'ev_q_function_output.pth')\n",
    "            benchmark = (avg_reward, sum(simul_results) / 100)\n",
    "        rewards_list = []\n",
    "        print()\n",
    "\n",
    "    \n",
    "    env.reset()\n",
    "    state = torch.tensor(env.game.get_network_input(), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    player_ind = random.randint(0, 3)\n",
    "    while env.game.current_player != player_ind:\n",
    "        move = env.game.sample_legal_move()\n",
    "        env.game.play_card(move)\n",
    "    \n",
    "    for t in count():\n",
    "        # Select action based on policy network\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values = policy_net(state)\n",
    "            action = q_values.max(1)[1].view(1, 1)\n",
    "\n",
    "        # Perform action in the environment\n",
    "        observation, reward, terminated = env.step(action.item(),fp=None)\n",
    "        rewards_list.append(reward)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated\n",
    "\n",
    "        # Compute next state\n",
    "        if not terminated:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store transition in memory\n",
    "        # int(env.game.turn_counter / 4)\n",
    "        memory[t].push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform optimization step\n",
    "        optimize_model()\n",
    "\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            # print(f'Episode {i_episode} ended in {t} steps.')\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "tensor([[ 0.0000,  1.0000, -1.1869],\n",
      "        [ 1.0000,  0.0000, -1.3314],\n",
      "        [ 2.0000,  0.0000, -1.3131],\n",
      "        [ 3.0000,  0.0000, -2.1086],\n",
      "        [ 4.0000,  0.0000, -3.4949],\n",
      "        [ 5.0000,  0.0000, -3.0317],\n",
      "        [ 6.0000,  0.0000, -1.5275],\n",
      "        [ 7.0000,  0.0000, -2.9876],\n",
      "        [ 8.0000,  0.0000, -2.5245],\n",
      "        [ 9.0000,  0.0000, -2.5576],\n",
      "        [10.0000,  0.0000, -1.5012],\n",
      "        [11.0000,  0.0000, -1.8418],\n",
      "        [12.0000,  0.0000, -0.9287],\n",
      "        [13.0000,  1.0000, -1.7412],\n",
      "        [14.0000,  0.0000, -1.1265],\n",
      "        [15.0000,  0.0000, -2.1763],\n",
      "        [16.0000,  1.0000, -1.9561],\n",
      "        [17.0000,  0.0000, -2.4696],\n",
      "        [18.0000,  0.0000, -2.5106],\n",
      "        [19.0000,  0.0000, -1.3729],\n",
      "        [20.0000,  1.0000, -1.1327],\n",
      "        [21.0000,  0.0000, -1.1238],\n",
      "        [22.0000,  0.0000, -2.3514],\n",
      "        [23.0000,  1.0000, -2.1013],\n",
      "        [24.0000,  1.0000, -0.9404],\n",
      "        [25.0000,  0.0000, -2.3779],\n",
      "        [26.0000,  0.0000, -1.5517],\n",
      "        [27.0000,  0.0000, -1.3506],\n",
      "        [28.0000,  1.0000, -1.8693],\n",
      "        [29.0000,  0.0000, -2.4621],\n",
      "        [30.0000,  0.0000, -3.1700],\n",
      "        [31.0000,  0.0000, -2.6200],\n",
      "        [32.0000,  1.0000, -0.3428],\n",
      "        [33.0000,  0.0000, -1.5477],\n",
      "        [34.0000,  0.0000, -1.1979],\n",
      "        [35.0000,  0.0000, -3.4005],\n",
      "        [36.0000,  1.0000, -2.0589],\n",
      "        [37.0000,  0.0000, -2.0292],\n",
      "        [38.0000,  1.0000, -0.7033],\n",
      "        [39.0000,  1.0000, -1.2922],\n",
      "        [40.0000,  0.0000, -1.4103],\n",
      "        [41.0000,  0.0000, -2.8561],\n",
      "        [42.0000,  0.0000, -0.9471],\n",
      "        [43.0000,  0.0000, -2.7854],\n",
      "        [44.0000,  1.0000, -0.7182],\n",
      "        [45.0000,  0.0000, -3.2967],\n",
      "        [46.0000,  0.0000, -1.5124],\n",
      "        [47.0000,  0.0000, -2.0868],\n",
      "        [48.0000,  0.0000, -3.3761],\n",
      "        [49.0000,  0.0000, -1.0934],\n",
      "        [50.0000,  1.0000, -1.3472],\n",
      "        [51.0000,  0.0000, -1.7773]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    env.game.reset()\n",
    "    net_input = env.game.get_network_input().to(device)\n",
    "    hand = net_input[:52]\n",
    "    cards = torch.arange(52).to(device)\n",
    "    q_func = policy_net(net_input)\n",
    "    print(f'{q_func.argmax()}')\n",
    "    print(torch.stack((cards, hand, q_func)).transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), 'ev_q_function_output.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average simulated game duration: 1.111\n",
      "Distribution of game lengths: [891, 107, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# simulate_game(policy_net, verbose=True, from_move=random.randint(0,3))\n",
    "simul_results = []\n",
    "with torch.no_grad():\n",
    "    simul_dist = [0 for i in range(13)]\n",
    "    for g in range(1000):\n",
    "        res = simulate_game(policy_net, verbose=False, from_move=random.randint(0,3))\n",
    "        simul_results.append(res)\n",
    "        simul_dist[res-1] += 1\n",
    "    print(f\"Average simulated game duration: {sum(simul_results) / 1000}\")\n",
    "    print(f\"Distribution of game lengths: {simul_dist}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new game as player 2 from turn 2.\n",
      "Player 0 plays ('10', 'D')\n",
      "Player 1 plays ('8', 'D')\n",
      "Player 2 plays ('J', 'D')\n",
      "Player 3 plays ('3', 'D')\n",
      "\n",
      "Tried to play illegal move ('J', 'D')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate_game(policy_net, verbose=True, from_move=random.randint(0, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

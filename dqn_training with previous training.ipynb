{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "oX3eusJ_wOOE"
   },
   "outputs": [],
   "source": [
    "from utils.card_engine import Card_Game, Card_Env, random_agent, get_legal_moves, net_policy, policy_legal_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-eHqM9YmwOOG",
    "outputId": "127c7506-b8d4-4abe-b32c-80972cdf9e29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7c2610107450>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import count\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axsV8YrowlxX",
    "outputId": "6aaacd73-7ed7-4ecd-e66d-76b27cf86680"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1h52hYwwOOI"
   },
   "source": [
    "# Replay Memory\n",
    "\n",
    "The replay memory is a list of 13 double ended queues (or deques) each of length 10000. An element in the i-th deque is a tuple, called a transition, consisting of the state of the particular game at the agent’s i-th turn, the action that the agent took at that state, the reward the agent received for taking the action, and the state that followed the agent’s action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ca2dtIVGwOOL"
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        # save a transition\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaHOzWTQwOOM"
   },
   "source": [
    "# Q network\n",
    "\n",
    "![hi](./figures/network-input-output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kZ8QrNClwOON"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    # n_input: the current state\n",
    "    #  (1x52)    +  (56x52)       +       (1x52): the current state\n",
    "    #    ^hand      ^who plays each card  ^cards not seen yet\n",
    "    #                       + cards played\n",
    "    # n_output: probability of playing each card\n",
    "    #   (1x52)\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_input, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dtype == torch.float32:\n",
    "            x = F.relu(self.layer1(x))\n",
    "            x = F.relu(self.layer2(x))\n",
    "        else:\n",
    "            x=x.to(torch.float32)\n",
    "            x = F.relu(self.layer1(x))\n",
    "            x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytAICBo-wOOP"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FKowXd9DwOOT",
    "outputId": "b863f755-2c9d-48eb-9c89-c61d8c2a271c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. GPU can be used.\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters and initializing the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Idw8ctzvwOOV"
   },
   "outputs": [],
   "source": [
    "EPS_START = 1\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 5000\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "MEMORY_SIZE = 10000\n",
    "\n",
    "# Learning rate of the optimizer\n",
    "LR = 1e-4\n",
    "\n",
    "# soft update rate\n",
    "TAU = 0.005\n",
    "\n",
    "# future discount\n",
    "GAMMA = 0.5\n",
    "\n",
    "\n",
    "env = Card_Env()\n",
    "state = env.game.get_network_input()\n",
    "\n",
    "n_input = len(state)\n",
    "n_actions = 52\n",
    "\n",
    "policy_net = DQN(n_input, n_actions).to(device)\n",
    "# use a target network to prevent oscillation or divergence\n",
    "target_net = DQN(n_input, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "# target_net.load_state_dict(torch.load('ev_working_function.pth'))\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = {turn: ReplayMemory(MEMORY_SIZE) for turn in range(13)}\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "foreign_weight = 'weights/ev_q_function_output_outside_folder.pth'\n",
    "\n",
    "foreign_net = DQN(n_input, n_actions).to(device)\n",
    "if device == torch.device('cpu'):\n",
    "    foreign_net.load_state_dict(torch.load(foreign_weight, map_location=torch.device('cpu')))\n",
    "else:\n",
    "    foreign_net.load_state_dict(torch.load(foreign_weight))\n",
    "\n",
    "env_against_net = Card_Env(foreign_policy= lambda game : net_policy(foreign_net, game))\n",
    "\n",
    "episode_durations = []\n",
    "episode_rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The $\\epsilon$-greedy policy used when training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_juKXZUKwOOW"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Given the game state, select an action by the epsilon-greedy policy\n",
    "'''\n",
    "def select_action(game):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "\n",
    "    # epsilon-greedy choice\n",
    "\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # return the index of the card with highest probability\n",
    "            # predicted from the policy net\n",
    "            return policy_legal_move(policy_net, game.get_network_input())\n",
    "    else:\n",
    "        # random select a legal action\n",
    "        return torch.tensor([[game.sample_legal_move()]], device=device, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_durations_rewards(show_result=False):\n",
    "    fig, ax = plt.subplots(2, 1)\n",
    "\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    rewards_t = torch.tensor(episode_rewards, dtype=torch.float)\n",
    "\n",
    "    title = 'Result' if show_result else 'Training...'\n",
    "    ax[0].set_title(f'{title} Duations each Episode')\n",
    "    ax[0].set_xlabel('Episode')\n",
    "    ax[0].set_ylabel('Duration')\n",
    "\n",
    "    ax[1].set_title(f'{title} Rewards each Episode')\n",
    "    ax[1].set_xlabel('Episode')\n",
    "    ax[1].set_ylabel('Reward')\n",
    "\n",
    "    ax[0].plot(durations_t.numpy())\n",
    "    ax[1].plot(rewards_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        ax[0].plot(means.numpy())\n",
    "\n",
    "    if len(rewards_t) >= 100:\n",
    "        means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        ax[1].plot(means.numpy())\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(fig)\n",
    "        else:\n",
    "            display.display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A single step optimization of the model using Deep Q-Learning\n",
    "1) samples a batch from memory, concatenates all the tensors into a single one\n",
    "2) computes $Q(s_t, a_t)$ and $V(s_{t+1}) = \\max_a Q(s_{t+1}, a)$, where $s_t$ --($a_t$)--> $s_{t+1}$\n",
    "3) computes the loss\n",
    "4) updates the target network (which is computing $V(s_{t+1}))$ at every step with soft update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GHpuHErGwOOR"
   },
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    transitions = []\n",
    "    for turn, mem in memory.items():\n",
    "        if len(mem) >= BATCH_SIZE:\n",
    "            transitions += mem.sample(BATCH_SIZE)\n",
    "    if transitions == []:\n",
    "        return\n",
    "\n",
    "    # transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # mask the non final states and find the corresponding next states\n",
    "    # We need an illegal move to be a non-final state\n",
    "    # Right now, we are throwing out all the final states which include the case when\n",
    "    # the agent ends the game prematurely after playing an illegal move\n",
    "\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    # print('non_final_mask is', non_final_mask)\n",
    "    non_final_next_states = [s for s in batch.next_state if s is not None]\n",
    "\n",
    "    non_final_next_states = torch.cat(non_final_next_states)\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # print('reward_batch is', reward_batch)\n",
    "\n",
    "\n",
    "    # compute Q(s_t, a)\n",
    "    # for each state in the batch, find the value of the corresponding action\n",
    "    state_action_values = policy_net(state_batch.to(torch.float)).gather(1, action_batch)\n",
    "\n",
    "    # compute V(s_{t+1}) = max_a Q(s_{t+1}, a) for the next_states using the target_net\n",
    "    next_state_values = torch.zeros(len(transitions), device=device)\n",
    "    # next_state_values = -10.0 * torch.ones(len(transitions), device=device)\n",
    "\n",
    "    if non_final_next_states != []:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # print(non_final_mask.shape, target_net(non_final_next_states).max(1).values)\n",
    "            next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values  # these will all be negative!\n",
    "\n",
    "    # R + \\gamma max_a Q(s', a)\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # compute the Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # back propagate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CCDMv6TuwOOU"
   },
   "outputs": [],
   "source": [
    "def move_to_card(move):\n",
    "    return ['2','3','4','5','6','7','8','9','10','J','Q','K','A'][move % 13], ['C', 'D', 'H', 'S'][int(move / 13)]\n",
    "\n",
    "def simulate_game(policy, verbose=False, from_move=0):\n",
    "    with torch.no_grad():\n",
    "        moves_played = 0\n",
    "        active_player = from_move % 4\n",
    "        if verbose:\n",
    "            print(f\"Starting new game as player {active_player} from turn {from_move}.\")\n",
    "        test_game = Card_Game()\n",
    "        for turn in range(52):\n",
    "            if test_game.current_player != active_player or turn < from_move:\n",
    "                move = test_game.sample_legal_move()\n",
    "            else:\n",
    "                # print(policy_net(test_game.get_network_input().to(device)))\n",
    "                moves_played += 1\n",
    "                move = policy_legal_move(policy_net, test_game.get_network_input()).item()\n",
    "                # move = policy_net(test_game.get_network_input().to(device)).argmax().item()\n",
    "                if not(move in test_game.get_legal_moves()):\n",
    "                    if verbose:\n",
    "                        print(f\"Tried to play illegal move {move_to_card(move)}\")\n",
    "                    return moves_played\n",
    "            if verbose:\n",
    "                print(f\"Player {test_game.current_player} plays {move_to_card(move)}\")\n",
    "            test_game.play_card(move)\n",
    "            if turn % 4 == 3 and verbose:\n",
    "                print()\n",
    "        return moves_played+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iKDssjijwOOX",
    "outputId": "6ac23bae-4969-423e-9563-03a0db8d267a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/txmszlou/Work/erdos-workshop/deep-learning/dqn-trick-game/utils/card_engine.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  card = torch.nn.functional.one_hot(torch.tensor(deck_index, dtype=int), self.num_cards * self.num_players).flatten()\n",
      "/tmp/ipykernel_3706547/2966759183.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(env_against_net.game.get_network_input(), dtype=torch.float32, device=device).unsqueeze(0)\n",
      "/tmp/ipykernel_3706547/2966759183.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Perform optimization step\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m target_net_state_dict \u001b[38;5;241m=\u001b[39m target_net\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[1;32m     78\u001b[0m policy_net_state_dict \u001b[38;5;241m=\u001b[39m policy_net\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "Cell \u001b[0;32mIn[16], line 43\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m non_final_next_states \u001b[38;5;241m!=\u001b[39m []:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;66;03m# print(non_final_mask.shape, target_net(non_final_next_states).max(1).values)\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m         next_state_values[non_final_mask] \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnon_final_next_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues  \u001b[38;5;66;03m# these will all be negative!\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# R + \\gamma max_a Q(s', a)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m expected_state_action_values \u001b[38;5;241m=\u001b[39m (next_state_values \u001b[38;5;241m*\u001b[39m GAMMA) \u001b[38;5;241m+\u001b[39m reward_batch\n",
      "File \u001b[0;32m~/.virtualenvs/dqn/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/dqn/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m, in \u001b[0;36mDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# print(f'x.shape = {x.shape}')\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# legal_moves_mask = get_legal_moves(x)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# legal_moves_mask = x[-52:]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# print(f'legal_moves_mask.shape = {legal_moves_mask.shape}')\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[0;32m---> 21\u001b[0m         x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     22\u001b[0m         x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x))\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.virtualenvs/dqn/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/dqn/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/dqn/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 5000\n",
    "update_ind = 100\n",
    "\n",
    "save_ind = 1000\n",
    "\n",
    "rewards_list = []\n",
    "benchmark = (-13, 0)\n",
    "\n",
    "for i_episode in count():\n",
    "    # if i_episode % save_ind == 0 and i_episode != 0:\n",
    "    #     torch.save(policy_net.state_dict(), f'/content/drive/MyDrive/deep-learning/dqn/q_fn-{i_episode}.pth')\n",
    "\n",
    "    if i_episode % update_ind == 0 and i_episode != 0:\n",
    "        plot_durations_rewards()\n",
    "        print(f\"Trained {i_episode} episodes\")\n",
    "        avg_reward = sum(rewards_list) / len(rewards_list)\n",
    "        print(f'Average reward per move: {avg_reward}.')\n",
    "        simul_results = []\n",
    "        simul_dist = [0 for i in range(14)]\n",
    "        for g in range(100):\n",
    "            res = simulate_game(policy_net, verbose=False, from_move=random.randint(0,3))\n",
    "            simul_results.append(res)\n",
    "            simul_dist[res-1] += 1\n",
    "        print(f\"Average simulated game duration: {sum(simul_results) / 100}\")\n",
    "        print(f\"Distribution of simulated game lengths: {simul_dist}\")\n",
    "        print(f\"Memory bank: {[len(mem) for i, mem in memory.items()]}\")\n",
    "        if benchmark[0] < avg_reward and i_episode >= num_episodes:\n",
    "            print(\"New benchmark set.\")\n",
    "            # torch.save(policy_net.state_dict(), 'weights/ev_q_function_output.pth')\n",
    "            benchmark = (avg_reward, sum(simul_results) / 100)\n",
    "        rewards_list = []\n",
    "        print()\n",
    "\n",
    "\n",
    "    env_against_net.reset()\n",
    "\n",
    "    player_ind = random.randint(0, 3)\n",
    "    while env_against_net.game.current_player != player_ind:\n",
    "        move = env_against_net.game.sample_legal_move()\n",
    "        env_against_net.game.play_card(move)\n",
    "\n",
    "    current_reward = 0\n",
    "    state = torch.tensor(env_against_net.game.get_network_input(), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    for t in count():\n",
    "        action = select_action(env_against_net.game)\n",
    "        # Select action based on policy network\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     q_values = policy_net(state)\n",
    "        #     action = q_values.max(1)[1].view(1, 1)\n",
    "\n",
    "        # Perform action in the environment\n",
    "        observation, reward, terminated = env_against_net.step(action.item())\n",
    "        rewards_list.append(reward)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated\n",
    "\n",
    "        current_reward += reward\n",
    "\n",
    "        # Compute next state\n",
    "        if not terminated:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store transition in memory\n",
    "        # int(env.game.turn_counter / 4)\n",
    "        memory[t].push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform optimization step\n",
    "        optimize_model()\n",
    "\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            # print(f'Episode {i_episode} ended in {t} steps.')\n",
    "            episode_durations.append(t+1)\n",
    "            episode_rewards.append(current_reward)\n",
    "            break\n",
    "\n",
    "plot_durations_rewards(show_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 955
    },
    "id": "LpRYtyXjwOOZ",
    "outputId": "6e951687-3c3d-49b0-fad0-b702eda41b2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(batch.state) = <class 'tuple'>\n",
      "state_batch.shape = torch.Size([100, 3016])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "unflatten: Provided sizes [52, 56] don't multiply up to the size of dim 0 (48) in the input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 42\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m reward_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(batch\u001b[38;5;241m.\u001b[39mreward)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# print('reward_batch is', reward_batch)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# compute Q(s_t, a)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# for each state in the batch, find the value of the corresponding action\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m state_action_values \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, action_batch)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_action_values.shape = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate_action_values\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# compute V(s_{t+1}) = max_a Q(s_{t+1}, a) for the next_states using the target_net\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/dqn/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/dqn/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m, in \u001b[0;36mDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     legal_moves_mask \u001b[38;5;241m=\u001b[39m \u001b[43mget_legal_moves\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[1;32m     18\u001b[0m         x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x))\n",
      "File \u001b[0;32m~/Work/erdos-workshop/deep-learning/dqn-trick-game/utils/card_engine.py:280\u001b[0m, in \u001b[0;36mget_legal_moves\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_legal_moves\u001b[39m(\u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m--> 280\u001b[0m     hand, state, winner_tracker \u001b[38;5;241m=\u001b[39m \u001b[43munpack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     current_suit \u001b[38;5;241m=\u001b[39m get_current_suit(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m current_suit \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Work/erdos-workshop/deep-learning/dqn-trick-game/utils/card_engine.py:231\u001b[0m, in \u001b[0;36munpack\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munpack\u001b[39m(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    229\u001b[0m     hand \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m52\u001b[39m]\n\u001b[0;32m--> 231\u001b[0m     state_winner \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m52\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m52\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m56\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m52\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m52\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m56\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     state \u001b[38;5;241m=\u001b[39m state_winner[:, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m52\u001b[39m]\n\u001b[1;32m    234\u001b[0m     winner_tracker \u001b[38;5;241m=\u001b[39m state_winner[:, \u001b[38;5;241m52\u001b[39m:]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: unflatten: Provided sizes [52, 56] don't multiply up to the size of dim 0 (48) in the input tensor"
     ]
    }
   ],
   "source": [
    "optimize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMG87Ll4wOOc",
    "outputId": "d64c7a3d-74f9-4fa1-9ad3-8a5c34d61c45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average simulated game duration of test: 11.1076\n",
      "Average simulated game duration of policy: 11.1212\n",
      "\n",
      "Distribution of game lengths of test: [70, 94, 230, 297, 375, 404, 530, 614, 600, 498, 471, 330, 320, 5167]\n",
      "Distribution of game lengths of policy: [77, 84, 225, 288, 416, 401, 545, 601, 540, 501, 459, 314, 342, 5207]\n"
     ]
    }
   ],
   "source": [
    "# simulate_game(policy_net, verbose=True, from_move=random.randint(0,3))\n",
    "test = DQN(n_input, n_actions).to(device)\n",
    "test.load_state_dict(torch.load('weights/ev_q_function_output.pth'))\n",
    "simul_test = []\n",
    "simul_policy = []\n",
    "with torch.no_grad():\n",
    "    test_dist = [0 for i in range(14)]\n",
    "    policy_dist = [0 for i in range(14)]\n",
    "    for g in range(10000):\n",
    "        res_policy = simulate_game(policy_net, verbose=False, from_move=random.randint(0,3))\n",
    "        res_test = simulate_game(policy_net, verbose=False, from_move=random.randint(0,3))\n",
    "        simul_test.append(res_test)\n",
    "        simul_policy.append(res_policy)\n",
    "        test_dist[res_test-1] += 1\n",
    "        policy_dist[res_policy-1] += 1\n",
    "    print(f\"Average simulated game duration of test: {sum(simul_test) / 10000}\")\n",
    "    print(f\"Average simulated game duration of policy: {sum(simul_policy) / 10000}\")\n",
    "    print()\n",
    "    print(f\"Distribution of game lengths of test: {test_dist}\")\n",
    "    print(f\"Distribution of game lengths of policy: {policy_dist}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59Lkdi84wOOa",
    "outputId": "38910f96-6230-4dc6-d7ac-3adc38f1564c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "tensor([[ 0.0000e+00,  1.0000e+00, -2.0753e+05],\n",
      "        [ 1.0000e+00,  1.0000e+00, -2.9378e+05],\n",
      "        [ 2.0000e+00,  0.0000e+00, -2.8098e+05],\n",
      "        [ 3.0000e+00,  0.0000e+00, -5.0699e+05],\n",
      "        [ 4.0000e+00,  0.0000e+00, -8.9135e+04],\n",
      "        [ 5.0000e+00,  0.0000e+00, -2.4677e+05],\n",
      "        [ 6.0000e+00,  1.0000e+00, -2.0640e+05],\n",
      "        [ 7.0000e+00,  0.0000e+00, -1.8064e+05],\n",
      "        [ 8.0000e+00,  0.0000e+00, -5.6643e+05],\n",
      "        [ 9.0000e+00,  1.0000e+00, -2.5549e+05],\n",
      "        [ 1.0000e+01,  0.0000e+00, -4.6659e+05],\n",
      "        [ 1.1000e+01,  0.0000e+00, -6.8289e+05],\n",
      "        [ 1.2000e+01,  0.0000e+00, -2.1979e+05],\n",
      "        [ 1.3000e+01,  0.0000e+00, -1.4135e+05],\n",
      "        [ 1.4000e+01,  1.0000e+00, -3.7770e+05],\n",
      "        [ 1.5000e+01,  0.0000e+00, -2.6255e+05],\n",
      "        [ 1.6000e+01,  0.0000e+00, -1.8918e+05],\n",
      "        [ 1.7000e+01,  1.0000e+00, -1.3708e+05],\n",
      "        [ 1.8000e+01,  0.0000e+00, -1.7210e+05],\n",
      "        [ 1.9000e+01,  1.0000e+00, -2.1982e+05],\n",
      "        [ 2.0000e+01,  0.0000e+00, -4.0011e+05],\n",
      "        [ 2.1000e+01,  0.0000e+00, -1.7736e+05],\n",
      "        [ 2.2000e+01,  0.0000e+00, -1.4170e+05],\n",
      "        [ 2.3000e+01,  1.0000e+00,  6.1781e+01],\n",
      "        [ 2.4000e+01,  1.0000e+00, -3.9792e+05],\n",
      "        [ 2.5000e+01,  1.0000e+00, -2.4140e+05],\n",
      "        [ 2.6000e+01,  0.0000e+00, -2.4501e+05],\n",
      "        [ 2.7000e+01,  0.0000e+00, -1.1705e+05],\n",
      "        [ 2.8000e+01,  0.0000e+00, -2.1068e+05],\n",
      "        [ 2.9000e+01,  0.0000e+00, -1.2997e+05],\n",
      "        [ 3.0000e+01,  0.0000e+00, -1.1886e+05],\n",
      "        [ 3.1000e+01,  0.0000e+00, -2.0973e+05],\n",
      "        [ 3.2000e+01,  0.0000e+00, -1.1543e+05],\n",
      "        [ 3.3000e+01,  0.0000e+00, -4.8126e+05],\n",
      "        [ 3.4000e+01,  0.0000e+00, -4.9614e+05],\n",
      "        [ 3.5000e+01,  0.0000e+00, -2.8299e+05],\n",
      "        [ 3.6000e+01,  0.0000e+00, -9.9079e+04],\n",
      "        [ 3.7000e+01,  0.0000e+00, -4.0834e+05],\n",
      "        [ 3.8000e+01,  0.0000e+00, -1.7844e+05],\n",
      "        [ 3.9000e+01,  0.0000e+00, -1.3227e+05],\n",
      "        [ 4.0000e+01,  0.0000e+00, -2.1762e+05],\n",
      "        [ 4.1000e+01,  0.0000e+00, -2.3043e+05],\n",
      "        [ 4.2000e+01,  0.0000e+00, -1.1957e+05],\n",
      "        [ 4.3000e+01,  0.0000e+00, -3.3597e+05],\n",
      "        [ 4.4000e+01,  1.0000e+00, -1.7312e+05],\n",
      "        [ 4.5000e+01,  0.0000e+00, -2.5242e+05],\n",
      "        [ 4.6000e+01,  0.0000e+00, -2.9285e+05],\n",
      "        [ 4.7000e+01,  0.0000e+00, -1.8822e+05],\n",
      "        [ 4.8000e+01,  1.0000e+00, -1.0084e+05],\n",
      "        [ 4.9000e+01,  0.0000e+00, -1.7458e+05],\n",
      "        [ 5.0000e+01,  0.0000e+00, -1.5757e+05],\n",
      "        [ 5.1000e+01,  1.0000e+00, -3.7745e+05]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    env.game.reset()\n",
    "    net_input = env.game.get_network_input().to(device)\n",
    "    hand = net_input[:52]\n",
    "    cards = torch.arange(52).to(device)\n",
    "    q_func_policy = policy_net(net_input)\n",
    "    # q_func_test = test(net_input)\n",
    "    print(f'{q_func_policy.argmax()}')\n",
    "    print(torch.stack((cards, hand, q_func_policy)).transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(memory_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outp:\n\u001b[0;32m      3\u001b[0m         pickle\u001b[38;5;241m.\u001b[39mdump(memory, outp, pickle\u001b[38;5;241m.\u001b[39mHIGHEST_PROTOCOL)\n\u001b[1;32m----> 5\u001b[0m \u001b[43msave_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msaved_memory_data.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[46], line 3\u001b[0m, in \u001b[0;36msave_memory\u001b[1;34m(memory_file)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_memory\u001b[39m(memory_file):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(memory_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outp:\n\u001b[1;32m----> 3\u001b[0m         \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHIGHEST_PROTOCOL\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yizhen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\storage.py:937\u001b[0m, in \u001b[0;36mTypedStorage.__reduce__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__reduce__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    936\u001b[0m     b \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[1;32m--> 937\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_use_new_zipfile_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (_load_from_bytes, (b\u001b[38;5;241m.\u001b[39mgetvalue(),))\n",
      "File \u001b[1;32mc:\\Users\\Yizhen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:633\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m--> 633\u001b[0m         \u001b[43m_legacy_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yizhen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:777\u001b[0m, in \u001b[0;36m_legacy_save\u001b[1;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m serialized_storage_keys:\n\u001b[0;32m    776\u001b[0m     storage, dtype \u001b[38;5;241m=\u001b[39m serialized_storages[key]\n\u001b[1;32m--> 777\u001b[0m     storage\u001b[38;5;241m.\u001b[39m_write_file(f, _should_read_directly(f), \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_element_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Yizhen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_utils.py:844\u001b[0m, in \u001b[0;36m_element_size\u001b[1;34m(dtype)\u001b[0m\n\u001b[0;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfinfo(dtype)\u001b[38;5;241m.\u001b[39mbits \u001b[38;5;241m>>\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[1;32m--> 844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mbits \u001b[38;5;241m>>\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbool:\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;66;03m# NOTE: torch.bool is not supported in torch.iinfo()\u001b[39;00m\n\u001b[0;32m    847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def save_memory(memory_file):\n",
    "    with open(memory_file, 'wb') as outp:\n",
    "        pickle.dump(memory, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "save_memory('saved_memory_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(memory_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m inp:\n\u001b[0;32m      3\u001b[0m         company1 \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(inp)\n\u001b[1;32m----> 5\u001b[0m load_memory(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_memory_data.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[80], line 3\u001b[0m, in \u001b[0;36mload_memory\u001b[1;34m(memory_file)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_memory\u001b[39m(memory_file):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(memory_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m inp:\n\u001b[1;32m----> 3\u001b[0m         company1 \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(inp)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_utils.py:202\u001b[0m, in \u001b[0;36m_rebuild_tensor_v2\u001b[1;34m(storage, storage_offset, size, stride, requires_grad, backward_hooks, metadata)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_rebuild_tensor_v2\u001b[39m(\n\u001b[0;32m    200\u001b[0m     storage, storage_offset, size, stride, requires_grad, backward_hooks, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    201\u001b[0m ):\n\u001b[1;32m--> 202\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m _rebuild_tensor(storage, storage_offset, size, stride)\n\u001b[0;32m    203\u001b[0m     tensor\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m requires_grad\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metadata:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_utils.py:181\u001b[0m, in \u001b[0;36m_rebuild_tensor\u001b[1;34m(storage, storage_offset, size, stride)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_rebuild_tensor\u001b[39m(storage, storage_offset, size, stride):\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# first construct a tensor with the correct dtype/device\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m0\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39mstorage\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mstorage\u001b[38;5;241m.\u001b[39m_untyped_storage\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mset_(storage\u001b[38;5;241m.\u001b[39m_untyped_storage, storage_offset, size, stride)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def load_memory(memory_file):\n",
    "    with open(memory_file, 'rb') as inp:\n",
    "        return = pickle.load(inp)\n",
    "\n",
    "memory = load_memory('saved_memory_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMqsy73rwOOb"
   },
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), 'ev_q_function_output.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aizTajupwOOc",
    "outputId": "b223fa47-b690-4c2b-a18d-3ff115bcf32d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new game as player 0 from turn 0.\n",
      "Player 0 plays ('J', 'D')\n",
      "Player 1 plays ('3', 'D')\n",
      "Player 2 plays ('A', 'D')\n",
      "Player 3 plays ('2', 'D')\n",
      "\n",
      "Player 2 plays ('4', 'C')\n",
      "Player 3 plays ('2', 'C')\n",
      "Player 0 plays ('J', 'C')\n",
      "Player 1 plays ('K', 'C')\n",
      "\n",
      "Player 0 plays ('6', 'C')\n",
      "Player 1 plays ('Q', 'C')\n",
      "Player 2 plays ('A', 'C')\n",
      "Player 3 plays ('10', 'C')\n",
      "\n",
      "Player 2 plays ('5', 'D')\n",
      "Player 3 plays ('K', 'D')\n",
      "Player 0 plays ('6', 'D')\n",
      "Player 1 plays ('7', 'D')\n",
      "\n",
      "Player 3 plays ('A', 'H')\n",
      "Player 0 plays ('6', 'H')\n",
      "Player 1 plays ('4', 'H')\n",
      "Player 2 plays ('Q', 'H')\n",
      "\n",
      "Player 3 plays ('5', 'S')\n",
      "Player 0 plays ('4', 'S')\n",
      "Player 1 plays ('8', 'S')\n",
      "Player 2 plays ('6', 'S')\n",
      "\n",
      "Player 1 plays ('7', 'S')\n",
      "Player 2 plays ('10', 'S')\n",
      "Player 3 plays ('2', 'S')\n",
      "Player 0 plays ('3', 'S')\n",
      "\n",
      "Player 2 plays ('9', 'S')\n",
      "Player 3 plays ('10', 'H')\n",
      "Player 0 plays ('J', 'S')\n",
      "Player 1 plays ('K', 'S')\n",
      "\n",
      "Player 0 plays ('J', 'H')\n",
      "Player 1 plays ('8', 'C')\n",
      "Player 2 plays ('7', 'C')\n",
      "Player 3 plays ('2', 'H')\n",
      "\n",
      "Player 0 plays ('9', 'D')\n",
      "Player 1 plays ('4', 'D')\n",
      "Player 2 plays ('9', 'C')\n",
      "Player 3 plays ('10', 'D')\n",
      "\n",
      "Player 0 plays ('3', 'H')\n",
      "Player 1 plays ('Q', 'S')\n",
      "Player 2 plays ('5', 'C')\n",
      "Player 3 plays ('5', 'H')\n",
      "\n",
      "Player 1 plays ('8', 'D')\n",
      "Player 2 plays ('3', 'C')\n",
      "Player 3 plays ('K', 'H')\n",
      "Player 0 plays ('9', 'H')\n",
      "\n",
      "Player 1 plays ('Q', 'D')\n",
      "Player 2 plays ('A', 'S')\n",
      "Player 3 plays ('8', 'H')\n",
      "Player 0 plays ('7', 'H')\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate_game(policy_net, verbose=True, from_move=random.randint(0, 3))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

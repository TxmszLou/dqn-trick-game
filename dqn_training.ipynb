{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from utils.card_engine import Card_Game, Card_Env, random_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "\n",
    "# global game gym\n",
    "env = Card_Env()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        # save a transition\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    # n_input: the current state\n",
    "    #  (1x52)    +  (56x52)       +       (1x52): the current state\n",
    "    #    ^hand       ^who plays each card  ^cards not seen yet\n",
    "    #                       + cards played\n",
    "    # n_output: probability of playing each card\n",
    "    #   (1x52)\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_input, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dtype == torch.float32:\n",
    "            x = F.relu(self.layer1(x))\n",
    "            x = F.relu(self.layer2(x))\n",
    "        else:\n",
    "            x=x.to(torch.float32)\n",
    "            x = F.relu(self.layer1(x))\n",
    "            x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The network agent\n",
    "Selects a move according to epsilon-greedy policy:\n",
    "sometimes uses the model to select move, sometimes just select one randomally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "\n",
    "# I am making batch_size small here so that we can test if this goes through in shorter time\n",
    "BATCH_SIZE = 3\n",
    "MEMORY_SIZE = 10000\n",
    "\n",
    "# Learning rate of the optimizer\n",
    "LR = 1e-4\n",
    "\n",
    "# soft update rate\n",
    "TAU = 0.005\n",
    "\n",
    "# future discount\n",
    "GAMMA = 1.0\n",
    "\n",
    "\n",
    "state = env.game.get_network_input()\n",
    "\n",
    "n_input = len(state)\n",
    "n_actions = 52\n",
    "\n",
    "policy_net = DQN(n_input, n_actions).to(device)\n",
    "# use a target network to prevent oscillation or divergence\n",
    "target_net = DQN(n_input, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(MEMORY_SIZE)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "'''\n",
    "Given the game state, select an action by the epsilon-greedy policy\n",
    "'''\n",
    "def select_action(game):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "\n",
    "    # epsilon-greedy choice\n",
    "        \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # return the index of the card with highest probability\n",
    "            # predicted from the policy net\n",
    "            print(policy_net(game.get_network_input().to(torch.float32)))\n",
    "            return policy_net(game.get_network_input().to(torch.float32)).max(0).indices.view(1,1)\n",
    "    else:\n",
    "        # random select a legal action\n",
    "        return torch.tensor([[game.sample_legal_move()]], device=device, dtype=torch.long) #changed from long\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A single step optimization of the model using Deep Q-Learning\n",
    "1) samples a batch from memory, concatenates all the tensors into a single one\n",
    "2) computes Q(s_t, a_t) and V(s_{t+1}) = max_a Q(s_{t+1}, a), where s_t --(a_t)--> s_{t+1}\n",
    "3) computes the loss\n",
    "4) updates the target network (which is computing V(s_{t+1})) at every step with soft update\n",
    "'''\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # mask the non final states and find the corresponding next states\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    if [s for s in batch.next_state if s is not None] == []:\n",
    "        return\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # compute Q(s_t, a)\n",
    "    # for each state in the batch, find the value of the corresponding action\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    # compute V(s_{t+1}) = max_a Q(s_{t+1}, a) for the next_states using the target_net\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "\n",
    "    # R + \\gamma max_a Q(s', a)\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # compute the Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # back propagate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kc/xzlz20857y957p_sy72nxl_c0000gn/T/ipykernel_71259/3442561819.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(game.get_network_input(), dtype=torch.long, device=device).unsqueeze(0)\n",
      "/var/folders/kc/xzlz20857y957p_sy72nxl_c0000gn/T/ipykernel_71259/3442561819.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_episode is 0\n",
      "t is  0\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        1, 0, 1, 1])\n",
      "tensor([0.1662, 0.1285, 0.1498, 0.1445, 0.1723, 0.1320, 0.1979, 0.0983, 0.0848,\n",
      "        0.1077, 0.1647, 0.1465, 0.1465, 0.1641, 0.1171, 0.0897, 0.1796, 0.1327,\n",
      "        0.0849, 0.1090, 0.1210, 0.1404, 0.1612, 0.1459, 0.1643, 0.1089, 0.1508,\n",
      "        0.1763, 0.1247, 0.1665, 0.1553, 0.0915, 0.1769, 0.0978, 0.1631, 0.1927,\n",
      "        0.1559, 0.0993, 0.0914, 0.1545, 0.2055, 0.0906, 0.1493, 0.1850, 0.1611,\n",
      "        0.1556, 0.1333, 0.1804, 0.1626, 0.1656, 0.1313, 0.1756])\n",
      "tensor([[40]])\n",
      "player plays an illegal move\n",
      "0-th game ends in 0 steps\n",
      "i_episode is 1\n",
      "t is  0\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1])\n",
      "tensor([0.1500, 0.1081, 0.1255, 0.1453, 0.1355, 0.1130, 0.1736, 0.0897, 0.1091,\n",
      "        0.1102, 0.0908, 0.1424, 0.1260, 0.1425, 0.0915, 0.1180, 0.1756, 0.0974,\n",
      "        0.0791, 0.1305, 0.0752, 0.1278, 0.1295, 0.1269, 0.1529, 0.0892, 0.1288,\n",
      "        0.2103, 0.1330, 0.1629, 0.1035, 0.0682, 0.1565, 0.1216, 0.1581, 0.1688,\n",
      "        0.1518, 0.0694, 0.1117, 0.1225, 0.1714, 0.1110, 0.1572, 0.1672, 0.1825,\n",
      "        0.1368, 0.1207, 0.1843, 0.1700, 0.1650, 0.0939, 0.1605])\n",
      "tensor([[27]])\n",
      "t is  1\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0])\n",
      "tensor([0.0776, 0.1035, 0.0729, 0.1410, 0.1474, 0.1213, 0.1849, 0.0334, 0.1702,\n",
      "        0.0458, 0.1443, 0.1046, 0.0879, 0.1269, 0.1241, 0.0532, 0.1833, 0.0917,\n",
      "        0.0933, 0.1446, 0.1433, 0.1180, 0.1269, 0.1347, 0.1523, 0.1001, 0.0760,\n",
      "        0.1649, 0.0895, 0.1093, 0.1897, 0.0688, 0.1333, 0.0445, 0.1345, 0.1802,\n",
      "        0.1115, 0.0542, 0.0790, 0.1652, 0.1031, 0.0916, 0.1305, 0.1786, 0.1374,\n",
      "        0.1302, 0.0865, 0.1728, 0.1472, 0.1254, 0.0725, 0.1657])\n",
      "tensor([[30]])\n",
      "player plays an illegal move\n",
      "1-th game ends in 1 steps\n",
      "i_episode is 2\n",
      "t is  0\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 0])\n",
      "tensor([0.1073, 0.1389, 0.0235, 0.1277, 0.1167, 0.0805, 0.1785, 0.0474, 0.0882,\n",
      "        0.0645, 0.1192, 0.1387, 0.1292, 0.1235, 0.1176, 0.0908, 0.1496, 0.1067,\n",
      "        0.0625, 0.1252, 0.0714, 0.1243, 0.1415, 0.1281, 0.1188, 0.0982, 0.0937,\n",
      "        0.1434, 0.1103, 0.1492, 0.1459, 0.1142, 0.1405, 0.0612, 0.1455, 0.1840,\n",
      "        0.1226, 0.0887, 0.0845, 0.1354, 0.1219, 0.1165, 0.1599, 0.1871, 0.1346,\n",
      "        0.1119, 0.0613, 0.1843, 0.1274, 0.1304, 0.0930, 0.1684])\n",
      "tensor([[43]])\n",
      "player plays an illegal move\n",
      "2-th game ends in 0 steps\n",
      "i_episode is 3\n",
      "t is  0\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 1, 0])\n",
      "tensor([[40]])\n",
      "t is  1\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1])\n",
      "tensor([0.0852, 0.1002, 0.0573, 0.1567, 0.1293, 0.1035, 0.1550, 0.0631, 0.1215,\n",
      "        0.0401, 0.1253, 0.0998, 0.1170, 0.1336, 0.1138, 0.0665, 0.1644, 0.0863,\n",
      "        0.0454, 0.0966, 0.0702, 0.1090, 0.1213, 0.1269, 0.1260, 0.0778, 0.0799,\n",
      "        0.1660, 0.1039, 0.1436, 0.1172, 0.0558, 0.1284, 0.0710, 0.1428, 0.1806,\n",
      "        0.1079, 0.0653, 0.0872, 0.1329, 0.1342, 0.0824, 0.1296, 0.1541, 0.0739,\n",
      "        0.1393, 0.1208, 0.1776, 0.1300, 0.1168, 0.0760, 0.1485])\n",
      "tensor([[35]])\n",
      "t is  2\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1])\n",
      "tensor([[27]])\n",
      "t is  3\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([0.2073, 0.1359, 0.3171, 0.1842, 0.2105, 0.2855, 0.1998, 0.1259, 0.2586,\n",
      "        0.1245, 0.1280, 0.2096, 0.1881, 0.2262, 0.1033, 0.1336, 0.2269, 0.1036,\n",
      "        0.2103, 0.1515, 0.2027, 0.2367, 0.1678, 0.1674, 0.2780, 0.2377, 0.1680,\n",
      "        0.1938, 0.1922, 0.1732, 0.1911, 0.2189, 0.2059, 0.2340, 0.1481, 0.1821,\n",
      "        0.2153, 0.0576, 0.0963, 0.2255, 0.2164, 0.1461, 0.1706, 0.2442, 0.1901,\n",
      "        0.1691, 0.2200, 0.2045, 0.2097, 0.2065, 0.1825, 0.1746])\n",
      "tensor([[2]])\n",
      "player plays an illegal move\n",
      "3-th game ends in 3 steps\n",
      "i_episode is 4\n",
      "t is  0\n",
      "tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0])\n",
      "tensor([0.1187, 0.1073, 0.0914, 0.1076, 0.1484, 0.1636, 0.2015, 0.0473, 0.1219,\n",
      "        0.0336, 0.1421, 0.1203, 0.1464, 0.1497, 0.0887, 0.0525, 0.1552, 0.0879,\n",
      "        0.0609, 0.1281, 0.1060, 0.1292, 0.1112, 0.1107, 0.1644, 0.0949, 0.1129,\n",
      "        0.1526, 0.0866, 0.1131, 0.1370, 0.0940, 0.1269, 0.0483, 0.1384, 0.1952,\n",
      "        0.1481, 0.0605, 0.0780, 0.1463, 0.1477, 0.0578, 0.1454, 0.1682, 0.1356,\n",
      "        0.1349, 0.1329, 0.1729, 0.1116, 0.1316, 0.0686, 0.1566])\n",
      "tensor([[6]])\n",
      "t is  1\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1])\n",
      "tensor([0.1359, 0.1034, 0.1926, 0.1846, 0.1625, 0.1591, 0.1837, 0.0904, 0.1865,\n",
      "        0.0900, 0.1031, 0.1922, 0.1502, 0.1529, 0.1065, 0.1265, 0.1917, 0.1018,\n",
      "        0.1273, 0.1401, 0.0809, 0.1275, 0.1344, 0.1412, 0.1652, 0.1522, 0.1534,\n",
      "        0.1901, 0.1647, 0.1742, 0.1437, 0.1397, 0.1446, 0.1604, 0.1730, 0.1896,\n",
      "        0.2007, 0.0651, 0.1169, 0.1579, 0.1739, 0.1084, 0.1670, 0.1580, 0.1323,\n",
      "        0.1638, 0.1999, 0.1796, 0.2042, 0.1920, 0.1084, 0.1609])\n",
      "tensor([[48]])\n",
      "player plays an illegal move\n",
      "4-th game ends in 1 steps\n",
      "i_episode is 5\n",
      "t is  0\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([0.0819, 0.1209, 0.0821, 0.1497, 0.1214, 0.0987, 0.1953, 0.0540, 0.1437,\n",
      "        0.0559, 0.0977, 0.1257, 0.1571, 0.1079, 0.1001, 0.0659, 0.1809, 0.1049,\n",
      "        0.0540, 0.1446, 0.1203, 0.1007, 0.1113, 0.1685, 0.1390, 0.0887, 0.1037,\n",
      "        0.1925, 0.1477, 0.1534, 0.1423, 0.0973, 0.1677, 0.0674, 0.1343, 0.1809,\n",
      "        0.0891, 0.0516, 0.0913, 0.1355, 0.1370, 0.1056, 0.1493, 0.1464, 0.1344,\n",
      "        0.1410, 0.1361, 0.1799, 0.1600, 0.1489, 0.0692, 0.1573])\n",
      "tensor([[6]])\n",
      "player plays an illegal move\n",
      "5-th game ends in 0 steps\n",
      "i_episode is 6\n",
      "t is  0\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1])\n",
      "tensor([[26]])\n",
      "t is  1\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
      "        0, 1, 0, 0])\n",
      "tensor([0.1506, 0.1648, 0.1517, 0.1996, 0.1847, 0.1872, 0.2073, 0.0699, 0.1907,\n",
      "        0.1007, 0.0659, 0.1752, 0.1963, 0.1548, 0.1215, 0.1506, 0.1835, 0.1143,\n",
      "        0.1671, 0.1499, 0.1026, 0.1633, 0.1541, 0.1998, 0.1843, 0.1272, 0.1814,\n",
      "        0.1701, 0.2169, 0.1824, 0.1412, 0.1372, 0.1687, 0.1523, 0.1608, 0.2088,\n",
      "        0.1427, 0.0820, 0.1252, 0.1473, 0.1613, 0.1068, 0.1562, 0.1621, 0.1106,\n",
      "        0.1657, 0.2272, 0.2104, 0.1949, 0.2117, 0.1132, 0.1711])\n",
      "tensor([[46]])\n",
      "t is  2\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([0.0977, 0.1367, 0.0716, 0.1190, 0.1607, 0.0943, 0.2125, 0.1089, 0.1164,\n",
      "        0.0507, 0.1887, 0.1050, 0.1357, 0.1503, 0.0836, 0.0645, 0.1624, 0.0916,\n",
      "        0.0620, 0.1119, 0.1311, 0.1064, 0.1321, 0.1429, 0.1716, 0.0956, 0.1025,\n",
      "        0.1528, 0.0949, 0.1570, 0.1532, 0.0907, 0.1698, 0.0662, 0.1412, 0.1715,\n",
      "        0.1385, 0.0670, 0.0746, 0.1488, 0.1484, 0.0792, 0.1284, 0.1530, 0.1309,\n",
      "        0.1617, 0.1093, 0.1610, 0.1072, 0.1403, 0.0889, 0.1758])\n",
      "tensor([[6]])\n",
      "t is  3\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0])\n",
      "tensor([0.1219, 0.1557, 0.0632, 0.1655, 0.1739, 0.1260, 0.2027, 0.0703, 0.1267,\n",
      "        0.0858, 0.1678, 0.1456, 0.1697, 0.1754, 0.1108, 0.1140, 0.1625, 0.1138,\n",
      "        0.1039, 0.1077, 0.0973, 0.1236, 0.1528, 0.1744, 0.1573, 0.1387, 0.1244,\n",
      "        0.1414, 0.1400, 0.2042, 0.1485, 0.1508, 0.1380, 0.1138, 0.1763, 0.1926,\n",
      "        0.1345, 0.1105, 0.1077, 0.1385, 0.1639, 0.1146, 0.1160, 0.1669, 0.0904,\n",
      "        0.1981, 0.1772, 0.1738, 0.1232, 0.1587, 0.1329, 0.1908])\n",
      "tensor([[29]])\n",
      "player plays an illegal move\n",
      "6-th game ends in 3 steps\n",
      "i_episode is 7\n",
      "t is  0\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0])\n",
      "tensor([0.1355, 0.1264, 0.1065, 0.1626, 0.1859, 0.1536, 0.2149, 0.0656, 0.1346,\n",
      "        0.1052, 0.1223, 0.1632, 0.1817, 0.1494, 0.0861, 0.1033, 0.1801, 0.0919,\n",
      "        0.0839, 0.1199, 0.1375, 0.1174, 0.1159, 0.1516, 0.1679, 0.0999, 0.1480,\n",
      "        0.1833, 0.1495, 0.1601, 0.1376, 0.1016, 0.1765, 0.0898, 0.1357, 0.2041,\n",
      "        0.1137, 0.0747, 0.1023, 0.1542, 0.1796, 0.1012, 0.1411, 0.1600, 0.1360,\n",
      "        0.1447, 0.1846, 0.1742, 0.1616, 0.1857, 0.1131, 0.1583])\n",
      "tensor([[6]])\n",
      "player plays an illegal move\n",
      "7-th game ends in 0 steps\n",
      "i_episode is 8\n",
      "t is  0\n",
      "tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 1, 0])\n",
      "tensor([0.1810, 0.1586, 0.1619, 0.1611, 0.2269, 0.2849, 0.2104, 0.0740, 0.1373,\n",
      "        0.1045, 0.0638, 0.1842, 0.2060, 0.1610, 0.0967, 0.1487, 0.1919, 0.0592,\n",
      "        0.1573, 0.1248, 0.1161, 0.1480, 0.1398, 0.1810, 0.2172, 0.1233, 0.1703,\n",
      "        0.2224, 0.1537, 0.1856, 0.1088, 0.1082, 0.1664, 0.1707, 0.1291, 0.2154,\n",
      "        0.1526, 0.1010, 0.1051, 0.1431, 0.1912, 0.1100, 0.1256, 0.1812, 0.1564,\n",
      "        0.1910, 0.2523, 0.2136, 0.1439, 0.2018, 0.1237, 0.1729])\n",
      "tensor([[5]])\n",
      "t is  1\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 1, 0, 1])\n",
      "tensor([0.0805, 0.1338, 0.0647, 0.1508, 0.0994, 0.0793, 0.1753, 0.0788, 0.1341,\n",
      "        0.0461, 0.1515, 0.1229, 0.1644, 0.1525, 0.0942, 0.1024, 0.1602, 0.1158,\n",
      "        0.0519, 0.1177, 0.0749, 0.1303, 0.1527, 0.1068, 0.1469, 0.1243, 0.0847,\n",
      "        0.1288, 0.1112, 0.1575, 0.1574, 0.1482, 0.1277, 0.0697, 0.1874, 0.1835,\n",
      "        0.1238, 0.0668, 0.0776, 0.1532, 0.1261, 0.1339, 0.1748, 0.1613, 0.1235,\n",
      "        0.1339, 0.0914, 0.1697, 0.1432, 0.1187, 0.0981, 0.1570])\n",
      "tensor([[34]])\n",
      "player plays an illegal move\n",
      "8-th game ends in 1 steps\n",
      "i_episode is 9\n",
      "t is  0\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0])\n",
      "tensor([0.1296, 0.1102, 0.1377, 0.1546, 0.1482, 0.1540, 0.2318, 0.0600, 0.1391,\n",
      "        0.0540, 0.0848, 0.1587, 0.1761, 0.1155, 0.0930, 0.0859, 0.1884, 0.1121,\n",
      "        0.0638, 0.1400, 0.1413, 0.1328, 0.1319, 0.1436, 0.1714, 0.0935, 0.1401,\n",
      "        0.2077, 0.1693, 0.1453, 0.1160, 0.0949, 0.1810, 0.0856, 0.1543, 0.2144,\n",
      "        0.1199, 0.0529, 0.0767, 0.1623, 0.1786, 0.1035, 0.1565, 0.1541, 0.1221,\n",
      "        0.1645, 0.1554, 0.2035, 0.1380, 0.1719, 0.0738, 0.1676])\n",
      "tensor([[6]])\n",
      "t is  1\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0])\n",
      "tensor([0.0755, 0.1152, 0.0615, 0.1679, 0.1077, 0.0986, 0.1599, 0.0508, 0.1325,\n",
      "        0.0695, 0.1175, 0.1291, 0.1469, 0.1323, 0.1400, 0.1282, 0.1814, 0.0980,\n",
      "        0.1077, 0.1158, 0.0401, 0.1335, 0.1535, 0.1278, 0.1110, 0.1244, 0.0838,\n",
      "        0.1672, 0.1329, 0.1597, 0.1540, 0.0917, 0.1409, 0.1105, 0.1481, 0.1839,\n",
      "        0.1065, 0.0849, 0.1280, 0.1293, 0.1248, 0.1126, 0.1431, 0.1727, 0.1316,\n",
      "        0.1451, 0.1372, 0.1979, 0.1515, 0.1723, 0.0935, 0.1464])\n",
      "tensor([[47]])\n",
      "t is  2\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0])\n",
      "tensor([0.1313, 0.1257, 0.1702, 0.1571, 0.1477, 0.1591, 0.1582, 0.1061, 0.1401,\n",
      "        0.0766, 0.0551, 0.1334, 0.1330, 0.1550, 0.1136, 0.1400, 0.1956, 0.0940,\n",
      "        0.1079, 0.1160, 0.1264, 0.1440, 0.1750, 0.1232, 0.1663, 0.1283, 0.0906,\n",
      "        0.1929, 0.1457, 0.1765, 0.1568, 0.1525, 0.1629, 0.1174, 0.1851, 0.2154,\n",
      "        0.1683, 0.0744, 0.0512, 0.1682, 0.1396, 0.1538, 0.1767, 0.1559, 0.1187,\n",
      "        0.1127, 0.1484, 0.1845, 0.1884, 0.1430, 0.1298, 0.1793])\n",
      "tensor([[35]])\n",
      "player plays an illegal move\n",
      "9-th game ends in 2 steps\n"
     ]
    }
   ],
   "source": [
    "# the main training loop\n",
    "if device != torch.device(\"cpu\"):\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 10\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # initialize the game\n",
    "    env.reset()\n",
    "    game = env.game\n",
    "    state = torch.tensor(game.get_network_input(), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    print('i_episode is', i_episode)\n",
    "\n",
    "    for t in count():\n",
    "        print('t is ', t)\n",
    "        # play until the game ends\n",
    "        if game.hands[game.current_player].sum(0) != 0:\n",
    "            print(game.hands[game.current_player])\n",
    "            action = select_action(game)\n",
    "            print(action)\n",
    "            observation, reward, terminated = env.step(action.item())\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            done = terminated\n",
    "        else:\n",
    "            terminated = True\n",
    "            done = terminated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            print(f'{i_episode}-th game ends in {t} steps')\n",
    "            break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

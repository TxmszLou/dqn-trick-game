{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.card_engine import Card_Game, Card_Env, random_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "\n",
    "# global game gym\n",
    "env = Card_Env()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        # save a transition\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    # n_input: the current state\n",
    "    #  (1x52)    +  (56x52)       +       (1x52): the current state\n",
    "    #    ^hand       ^who plays each card  ^cards not seen yet\n",
    "    #                       + cards played\n",
    "    # n_output: probability of playing each card\n",
    "    #   (1x52)\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_input, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The network agent\n",
    "Selects a move according to epsilon-greedy policy:\n",
    "sometimes uses the model to select move, sometimes just select one randomally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "\n",
    "# I am making batch_size small here so that we can test if this goes through in shorter time\n",
    "BATCH_SIZE = 3\n",
    "MEMORY_SIZE = 10000\n",
    "\n",
    "# Learning rate of the optimizer\n",
    "LR = 1e-4\n",
    "\n",
    "# soft update rate\n",
    "TAU = 0.005\n",
    "\n",
    "# future discount\n",
    "GAMMA = 1.0\n",
    "\n",
    "\n",
    "state = game.get_network_input()\n",
    "\n",
    "n_input = len(state)\n",
    "n_actions = 52\n",
    "\n",
    "policy_net = DQN(n_input, n_actions).to(device)\n",
    "# use a target network to prevent oscillation or divergence\n",
    "target_net = DQN(n_input, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(MEMORY_SIZE)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "'''\n",
    "Given the game state, select an action by the epsilon-greedy policy\n",
    "'''\n",
    "def select_action(game):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "\n",
    "    # epsilon-greedy choice\n",
    "        \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # return the index of the card with highest probability\n",
    "            # predicted from the policy net\n",
    "            print(policy_net(game.get_network_input().to(torch.float32)))\n",
    "            return policy_net(game.get_network_input().to(torch.float32)).max(0).indices.view(1,1)\n",
    "    else:\n",
    "        # random select a legal action\n",
    "        return torch.tensor([[game.sample_legal_move()]], device=device, dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A single step optimization of the model using Deep Q-Learning\n",
    "1) samples a batch from memory, concatenates all the tensors into a single one\n",
    "2) computes Q(s_t, a_t) and V(s_{t+1}) = max_a Q(s_{t+1}, a), where s_t --(a_t)--> s_{t+1}\n",
    "3) computes the loss\n",
    "4) updates the target network (which is computing V(s_{t+1})) at every step with soft update\n",
    "'''\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # mask the non final states and find the corresponding next states\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # compute Q(s_t, a)\n",
    "    # for each state in the batch, find the value of the corresponding action\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    # compute V(s_{t+1}) = max_a Q(s_{t+1}, a) for the next_states using the target_net\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "\n",
    "    # R + \\gamma max_a Q(s', a)\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # compute the Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # back propagate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corpt\\AppData\\Local\\Temp\\ipykernel_6872\\1964924289.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(game.get_network_input(), dtype=torch.long, device=device).unsqueeze(0)\n",
      "C:\\Users\\corpt\\AppData\\Local\\Temp\\ipykernel_6872\\1964924289.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_episode is 0\n",
      "t is  0\n",
      "tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[32]])\n",
      "t is  1\n",
      "tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[6]])\n",
      "t is  2\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0])\n",
      "tensor([[7]])\n",
      "t is  3\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0])\n",
      "tensor([[22]])\n",
      "t is  4\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1])\n",
      "tensor([[39]])\n",
      "t is  5\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[8]])\n",
      "t is  6\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0])\n",
      "tensor([[16]])\n",
      "t is  7\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1])\n",
      "tensor([[49]])\n",
      "t is  8\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1])\n",
      "tensor([[31]])\n",
      "t is  9\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([ 0.0878, -0.0826, -0.0116, -0.0082,  0.0019, -0.0222, -0.0622,  0.1369,\n",
      "        -0.0348, -0.0628,  0.0603, -0.0498,  0.0196, -0.0116,  0.0460, -0.1003,\n",
      "         0.0113,  0.0649,  0.0483, -0.0052,  0.0148,  0.0378,  0.0710,  0.0441,\n",
      "         0.0017,  0.0173,  0.0273, -0.0013, -0.0442,  0.0266, -0.0275,  0.0467,\n",
      "         0.0855, -0.0051, -0.0353,  0.0784,  0.0795,  0.0085, -0.0317, -0.1089,\n",
      "        -0.0830, -0.0134,  0.0786,  0.0490,  0.0064,  0.0042, -0.0504, -0.0198,\n",
      "        -0.0578,  0.0248, -0.0609,  0.1088])\n",
      "tensor([[7]])\n",
      "player plays an illegal move\n",
      "0-th game ends in 9 steps\n",
      "i_episode is 1\n",
      "t is  0\n",
      "tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[34]])\n",
      "t is  1\n",
      "tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([ 0.0792, -0.0892, -0.0308, -0.0285,  0.0035, -0.0522, -0.0759,  0.1018,\n",
      "        -0.0595, -0.0832,  0.0653, -0.0490,  0.0436, -0.0169,  0.0394, -0.0792,\n",
      "        -0.0070,  0.0621,  0.0407, -0.0201,  0.0226,  0.0429,  0.0595,  0.0585,\n",
      "         0.0170,  0.0464,  0.0018, -0.0036, -0.0237,  0.0231, -0.0160,  0.0411,\n",
      "         0.0862, -0.0081, -0.0354,  0.0787,  0.0718, -0.0141, -0.0291, -0.0914,\n",
      "        -0.0667, -0.0255,  0.0874,  0.0290, -0.0142,  0.0049, -0.0302, -0.0216,\n",
      "        -0.0805,  0.0232, -0.0606,  0.0788])\n",
      "tensor([[7]])\n",
      "player plays an illegal move\n",
      "1-th game ends in 1 steps\n",
      "i_episode is 2\n",
      "t is  0\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[12]])\n",
      "t is  1\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[14]])\n",
      "t is  2\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[45]])\n",
      "t is  3\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0])\n",
      "tensor([[0]])\n",
      "t is  4\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[32]])\n",
      "t is  5\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0])\n",
      "tensor([[40]])\n",
      "t is  6\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[27]])\n",
      "t is  7\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[22]])\n",
      "t is  8\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[47]])\n",
      "t is  9\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[26]])\n",
      "t is  10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[15]])\n",
      "t is  11\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0])\n",
      "tensor([[4]])\n",
      "t is  12\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[13]])\n",
      "t is  13\n",
      "2-th game ends in 13 steps\n",
      "i_episode is 3\n",
      "t is  0\n",
      "tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0])\n",
      "tensor([ 0.0785, -0.0935, -0.0310, -0.0202,  0.0048, -0.0530, -0.0544,  0.0864,\n",
      "        -0.0486, -0.0864,  0.0717, -0.0427,  0.0580, -0.0225,  0.0456, -0.0726,\n",
      "        -0.0083,  0.0634,  0.0226, -0.0206,  0.0141,  0.0441,  0.0843,  0.0478,\n",
      "         0.0114,  0.0409,  0.0031,  0.0179, -0.0281,  0.0329, -0.0166,  0.0461,\n",
      "         0.1083,  0.0094, -0.0264,  0.0805,  0.0582, -0.0110, -0.0627, -0.0713,\n",
      "        -0.0751, -0.0255,  0.1077,  0.0331, -0.0111,  0.0106, -0.0326, -0.0239,\n",
      "        -0.0716,  0.0378, -0.0395,  0.0818])\n",
      "tensor([[32]])\n",
      "t is  1\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        1, 0, 1, 1])\n",
      "tensor([[42]])\n",
      "t is  2\n",
      "tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[44]])\n",
      "t is  3\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1])\n",
      "tensor([[48]])\n",
      "t is  4\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1])\n",
      "tensor([[5]])\n",
      "t is  5\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[19]])\n",
      "t is  6\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[10]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t is  7\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1])\n",
      "tensor([ 0.0853, -0.1006, -0.0267, -0.0158, -0.0011, -0.0487, -0.0692,  0.1042,\n",
      "        -0.0404, -0.0810,  0.0497, -0.0486,  0.0362, -0.0239,  0.0524, -0.0894,\n",
      "        -0.0193,  0.0584,  0.0368,  0.0009,  0.0115,  0.0259,  0.0614,  0.0273,\n",
      "         0.0085,  0.0386,  0.0180,  0.0228, -0.0266,  0.0582, -0.0168,  0.0665,\n",
      "         0.0889,  0.0077, -0.0067,  0.0595,  0.0633,  0.0079, -0.0436, -0.0999,\n",
      "        -0.0903, -0.0041,  0.0563,  0.0307, -0.0142,  0.0111, -0.0348, -0.0214,\n",
      "        -0.0654,  0.0374, -0.0729,  0.0930])\n",
      "tensor([[7]])\n",
      "player plays an illegal move\n",
      "3-th game ends in 7 steps\n",
      "i_episode is 4\n",
      "t is  0\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
      "        0, 1, 1, 1])\n",
      "tensor([[49]])\n",
      "t is  1\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
      "        0, 0, 1, 1])\n",
      "tensor([[42]])\n",
      "t is  2\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([ 0.0838, -0.0954, -0.0254, -0.0256,  0.0233, -0.0503, -0.0513,  0.0952,\n",
      "        -0.0453, -0.0870,  0.0614, -0.0346,  0.0481, -0.0327,  0.0492, -0.0631,\n",
      "         0.0042,  0.0629,  0.0290, -0.0261,  0.0106,  0.0338,  0.0708,  0.0447,\n",
      "        -0.0066,  0.0398,  0.0026,  0.0200, -0.0326,  0.0522, -0.0188,  0.0610,\n",
      "         0.1101,  0.0072, -0.0137,  0.0719,  0.0640, -0.0099, -0.0446, -0.0923,\n",
      "        -0.0700, -0.0293,  0.0880,  0.0331, -0.0071,  0.0068, -0.0395, -0.0139,\n",
      "        -0.0669,  0.0316, -0.0475,  0.0917])\n",
      "tensor([[32]])\n",
      "player plays an illegal move\n",
      "4-th game ends in 2 steps\n",
      "i_episode is 5\n",
      "t is  0\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0])\n",
      "tensor([ 0.0717, -0.1019, -0.0234, -0.0207,  0.0155, -0.0362, -0.0340,  0.0796,\n",
      "        -0.0386, -0.0781,  0.0667, -0.0406,  0.0558, -0.0294,  0.0393, -0.0655,\n",
      "         0.0007,  0.0752,  0.0410, -0.0088, -0.0005,  0.0358,  0.0813,  0.0288,\n",
      "        -0.0007,  0.0341,  0.0111,  0.0196, -0.0329,  0.0504, -0.0288,  0.0581,\n",
      "         0.1139,  0.0046, -0.0025,  0.0674,  0.0738, -0.0058, -0.0641, -0.0803,\n",
      "        -0.0857, -0.0213,  0.0987,  0.0302, -0.0082,  0.0028, -0.0454, -0.0126,\n",
      "        -0.0659,  0.0305, -0.0428,  0.0946])\n",
      "tensor([[32]])\n",
      "player plays an illegal move\n",
      "5-th game ends in 0 steps\n",
      "i_episode is 6\n",
      "t is  0\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[29]])\n",
      "t is  1\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
      "        1, 0, 1, 0])\n",
      "tensor([[12]])\n",
      "t is  2\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
      "        1, 0, 1, 0])\n",
      "tensor([[18]])\n",
      "t is  3\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
      "        1, 0, 1, 0])\n",
      "tensor([[46]])\n",
      "t is  4\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        1, 0, 1, 0])\n",
      "tensor([[22]])\n",
      "t is  5\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[37]])\n",
      "t is  6\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[17]])\n",
      "t is  7\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[8]])\n",
      "t is  8\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 1, 0])\n",
      "tensor([ 0.0831, -0.1126, -0.0365, -0.0214,  0.0206, -0.0345, -0.0352,  0.0928,\n",
      "        -0.0480, -0.0822,  0.0864, -0.0587,  0.0732, -0.0402,  0.0338, -0.0712,\n",
      "         0.0028,  0.0508,  0.0085, -0.0123,  0.0317,  0.0515,  0.0647,  0.0634,\n",
      "         0.0074,  0.0339, -0.0027,  0.0060, -0.0496,  0.0513, -0.0293,  0.0650,\n",
      "         0.1367, -0.0038, -0.0124,  0.0722,  0.0615, -0.0004, -0.0402, -0.0854,\n",
      "        -0.0444, -0.0246,  0.0948,  0.0197, -0.0092, -0.0047, -0.0144, -0.0133,\n",
      "        -0.0528,  0.0250, -0.0777,  0.1041])\n",
      "tensor([[32]])\n",
      "player plays an illegal move\n",
      "6-th game ends in 8 steps\n",
      "i_episode is 7\n",
      "t is  0\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0])\n",
      "tensor([[49]])\n",
      "t is  1\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([ 0.1046, -0.1120, -0.0230, -0.0140,  0.0215, -0.0494, -0.0632,  0.0841,\n",
      "        -0.0565, -0.0736,  0.0662, -0.0332,  0.0670, -0.0237,  0.0445, -0.0799,\n",
      "         0.0009,  0.0468,  0.0303, -0.0267,  0.0227,  0.0513,  0.0766,  0.0435,\n",
      "         0.0091,  0.0345,  0.0107,  0.0061, -0.0319,  0.0494, -0.0360,  0.0759,\n",
      "         0.1039,  0.0043, -0.0204,  0.0679,  0.0544, -0.0017, -0.0746, -0.0723,\n",
      "        -0.0609, -0.0276,  0.0873,  0.0355,  0.0182,  0.0109, -0.0483, -0.0184,\n",
      "        -0.0606,  0.0278, -0.0585,  0.0821])\n",
      "tensor([[0]])\n",
      "player plays an illegal move\n",
      "7-th game ends in 1 steps\n",
      "i_episode is 8\n",
      "t is  0\n",
      "tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1])\n",
      "tensor([ 0.0731, -0.1065, -0.0008, -0.0180,  0.0244, -0.0427, -0.0435,  0.0789,\n",
      "        -0.0465, -0.0832,  0.0657, -0.0323,  0.0682, -0.0312,  0.0406, -0.0710,\n",
      "         0.0164,  0.0841,  0.0136, -0.0257, -0.0087,  0.0389,  0.0792,  0.0402,\n",
      "         0.0057,  0.0247,  0.0093,  0.0190, -0.0348,  0.0527, -0.0298,  0.0671,\n",
      "         0.1357, -0.0021, -0.0046,  0.0770,  0.0806, -0.0081, -0.0589, -0.0619,\n",
      "        -0.0684, -0.0312,  0.1045,  0.0179,  0.0013,  0.0020, -0.0279, -0.0113,\n",
      "        -0.0621,  0.0280, -0.0382,  0.0705])\n",
      "tensor([[32]])\n",
      "player plays an illegal move\n",
      "8-th game ends in 0 steps\n",
      "i_episode is 9\n",
      "t is  0\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[0]])\n",
      "t is  1\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0])\n",
      "tensor([[4]])\n",
      "t is  2\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[41]])\n",
      "t is  3\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1])\n",
      "tensor([[24]])\n",
      "t is  4\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0])\n",
      "tensor([[48]])\n",
      "t is  5\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0])\n",
      "tensor([[2]])\n",
      "t is  6\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[21]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t is  7\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1])\n",
      "tensor([[27]])\n",
      "t is  8\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[11]])\n",
      "t is  9\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([ 0.0839, -0.0897,  0.0003, -0.0139,  0.0306, -0.0382, -0.0491,  0.0910,\n",
      "        -0.0302, -0.0844,  0.0739, -0.0510,  0.0691, -0.0222,  0.0490, -0.0868,\n",
      "         0.0103,  0.0708,  0.0008, -0.0110, -0.0044,  0.0465,  0.0987,  0.0265,\n",
      "         0.0093,  0.0518,  0.0116,  0.0347, -0.0579,  0.0488, -0.0247,  0.0631,\n",
      "         0.1423,  0.0092, -0.0132,  0.0701,  0.0695, -0.0086, -0.0487, -0.0865,\n",
      "        -0.0396, -0.0209,  0.1189,  0.0101, -0.0022,  0.0007, -0.0376, -0.0031,\n",
      "        -0.0534,  0.0349, -0.0492,  0.0989])\n",
      "tensor([[32]])\n",
      "player plays an illegal move\n",
      "9-th game ends in 9 steps\n"
     ]
    }
   ],
   "source": [
    "# the main training loop\n",
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 10\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # initialize the game\n",
    "    env.reset()\n",
    "    game = env.game\n",
    "    state = torch.tensor(game.get_network_input(), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    print('i_episode is', i_episode)\n",
    "\n",
    "    for t in count():\n",
    "        print('t is ', t)\n",
    "        # play until the game ends\n",
    "        if game.hands[game.current_player].sum(0) != 0:\n",
    "            print(game.hands[game.current_player])\n",
    "            action = select_action(game)\n",
    "            print(action)\n",
    "            observation, reward, terminated = env.step(action.item())\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            done = terminated\n",
    "        else:\n",
    "            terminated = True\n",
    "            done = terminated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            print(f'{i_episode}-th game ends in {t} steps')\n",
    "            break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
